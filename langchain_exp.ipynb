{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API credentials\n",
    "with open('api_key.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "os.environ['OPENAI_API_KEY'] = config['OPEN_AI_KEY']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = config['HUGGING_FACE_TOKEN_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "### - Models\n",
    "    - LLMs: 20+ integrations models\n",
    "    - Chat Models\n",
    "    - Text Embedding models: 10+ integrations\n",
    "\n",
    "### - Prompts\n",
    "    - Prompt Templates\n",
    "    - Output Parsers: 5+ implementations\n",
    "        - Retry/fixing logic\n",
    "    - Example Selectors: 5+ implementations\n",
    "\n",
    "### - Indexes\n",
    "    - Document Loaders: 50+ implementations\n",
    "    - Text Splitters: 10+ implementations\n",
    "    - Vector stores\n",
    "    - Retrievers\n",
    "\n",
    "### - Chains\n",
    "    - Prompt + LLM + Output parsing\n",
    "    - Can be used as building blocks for longer chains\n",
    "    - More application specific chains: 20 + types\n",
    "\n",
    "### - Agents\n",
    "    - Agent types: 5+\n",
    "        - Algorithms for getting LLMs to use tools\n",
    "    - Agent toolkits: 10+\n",
    "        - Agents armed with specific tools for a specific application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it is now there are six models being covered in Longchain\n",
    "# LLMs and prompts\n",
    "# Chains\n",
    "# Data Augmented Generation\n",
    "# Agents\n",
    "# Memory\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Prompts\n",
    "LLMs take a string as an input (prompt) and output a string (completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"text-davinci-003\"\n",
    ")\n",
    "\n",
    "llm_hugging_face = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-xl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM takes a prompt as an input and outputs a completion\n",
    "# prompt = \"My name is Jerry and I am looking for a senior data scientist or machine learning engineer job\"\n",
    "# completion = llm_hugging_face(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 391kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 95.3kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 5.31MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 204kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 38.7kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 9.82MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:01<00:00, 71.1MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 27.0kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 55.9kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 13.3MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 175kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 4.38MB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 9.82MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 116kB/s]\n"
     ]
    }
   ],
   "source": [
    "# embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings_hugging_face = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06501688808202744, -0.04788777232170105, 0.051248155534267426, 0.06370953470468521, -0.020586827769875526, -0.13232406973838806, -0.025029074400663376, -0.06336953490972519, -0.07616551220417023, -0.05247809365391731, -0.07098755240440369, -0.03720574080944061, 0.03114102967083454, -0.0573573000729084, -0.034847892820835114, 0.0413450226187706, -0.048038505017757416, -0.01105794683098793, 0.02295728586614132, -0.13795898854732513, 0.01054036058485508, 0.004916073754429817, -0.018240058794617653, -0.11778751015663147, 0.00014174864918459207, -0.00906381756067276, 0.03587860241532326, 0.049300871789455414, 0.0018293778412044048, 0.011244402267038822, -0.04029207304120064, -0.01543509867042303, 0.03291197866201401, 0.10186728835105896, 0.02767772413790226, 0.05389564111828804, -0.05304254591464996, 0.014578720554709435, 0.03618146851658821, 0.04540051147341728, -0.032780278474092484, -0.002884884597733617, -0.07596230506896973, -0.04300385341048241, -0.052731454372406006, 0.02202579937875271, -0.00264448719099164, -0.1396389901638031, 0.09070712327957153, -0.041942719370126724, -0.0980236828327179, -0.045089300721883774, 0.05969260632991791, 0.0019365822663530707, -0.022275486961007118, -0.007549626287072897, 0.06836681813001633, 0.009234198369085789, -0.03556796535849571, 0.0013123820535838604, -0.025660010054707527, -0.02573871612548828, -0.07437139004468918, 0.008057172410190105, -0.015706336125731468, -0.012133573181927204, 0.028306279331445694, 0.08229709416627884, 0.03410656750202179, -0.1837361454963684, -0.020453868433833122, 0.010272076353430748, -0.08614423871040344, 0.0032447201665490866, 0.05517352744936943, -0.021764084696769714, 0.06779200583696365, 0.02197706513106823, 0.12147067487239838, 0.0134851960465312, 0.0076145450584590435, -0.07201262563467026, -0.06792134791612625, 0.045356061309576035, -0.023416949436068535, 0.011865011416375637, -0.07003025710582733, 0.07526055723428726, -0.000688074273057282, -0.043117888271808624, -7.692926010349765e-05, -0.03976646065711975, -0.010320798493921757, 0.024499135091900826, -0.03027907945215702, -0.011983507312834263, 0.00440657464787364, 0.0498310849070549, -0.061809562146663666, 0.030156340450048447, -0.05976938083767891, -0.028667036443948746, 0.06566520035266876, 0.047484252601861954, 0.0152455298230052, 0.08481191843748093, -0.07579459249973297, 0.05765480175614357, 0.011565198190510273, 0.0026701721362769604, -0.02497166581451893, 0.0292319655418396, -0.0580926388502121, 0.0656917542219162, -0.005739786196500063, -0.015354641713202, -0.07778124511241913, 0.034489281475543976, 0.004014858976006508, -0.009981323964893818, -0.04192056134343147, 0.05431178957223892, -0.10532067716121674, 0.028309667482972145, 0.021756108850240707, -0.06135861575603485, -0.06210995465517044, -2.8259409492805815e-33, 0.0439031757414341, 0.0009324733400717378, 0.05406700074672699, -0.019480198621749878, 0.021837439388036728, -0.019782256335020065, -0.020847130566835403, 0.06757233291864395, 0.03303278982639313, 0.014859456568956375, -0.05724364519119263, 0.040010031312704086, -0.043295495212078094, 0.035258322954177856, -0.09836220741271973, 0.024630442261695862, 0.0004772153915837407, 0.0013256208039820194, -0.10985062271356583, 0.040428388863801956, 0.017443032935261726, -0.06903164833784103, -0.026144955307245255, 0.04219334200024605, -0.022618841379880905, 0.010059433989226818, -0.017695238813757896, -0.037207312881946564, 0.10963460803031921, 0.037469666451215744, -0.023983199149370193, -0.02423936501145363, -0.0018299848306924105, 0.011569665744900703, 0.058344848453998566, 0.007742159999907017, -0.027009589597582817, -0.03278525546193123, 0.020292894914746284, 0.026711709797382355, -0.013516034930944443, 0.04240681976079941, 0.05167117342352867, -0.004911947995424271, -0.027818400412797928, -0.049556855112314224, 0.11178914457559586, -0.05452686548233032, 0.12761175632476807, 0.026305826380848885, -0.07418466359376907, -0.020089346915483475, 0.006358471233397722, 0.0006247590645216405, -0.0014145586173981428, 0.04150225222110748, 0.07378443330526352, -0.00619652820751071, 0.008873595856130123, 0.07933005690574646, -0.08905015140771866, 0.07566255331039429, -0.03127184137701988, 0.019390741363167763, -0.014436638914048672, -0.03725384175777435, 0.052267979830503464, -0.008799062110483646, 0.055788084864616394, 0.07739084213972092, 0.04309955984354019, -0.008337886072695255, 0.11464429646730423, 0.026579922065138817, 0.06280818581581116, 0.021330449730157852, -0.02098357491195202, -0.054622236639261246, 0.019991034641861916, 0.005100575275719166, 0.07054760307073593, -0.023911142721772194, 0.0017720501637086272, 0.02901630848646164, 0.11136165261268616, 0.06430884450674057, -0.03774719312787056, -0.033890414983034134, -0.002668441040441394, 0.011199883185327053, -0.10930721461772919, -0.01592094451189041, 0.05206013470888138, 0.03959853947162628, -0.025119727477431297, -1.1095763928835161e-33, -0.04085777699947357, 0.011771902441978455, 0.022526828572154045, 0.06094017252326012, 0.09378871321678162, 0.0058002169243991375, 0.07802128791809082, -0.026897354051470757, 0.047427356243133545, 0.004615278914570808, -0.00528442719951272, 0.009093684144318104, 0.007659397087991238, 0.010143681429326534, -0.010373730212450027, 0.10493206232786179, -0.09413179010152817, 0.02492520585656166, -0.13026924431324005, -0.024147525429725647, -0.045892953872680664, 0.10029539465904236, -0.11607344448566437, 0.0424344576895237, 0.0800890401005745, -0.058107197284698486, 0.007757697254419327, 0.014448623172938824, -0.00844950694590807, 0.07177330553531647, -0.09867145121097565, -0.004462967626750469, -0.04805124178528786, -0.015794899314641953, -0.041502080857753754, -0.03667960315942764, 0.019231989979743958, 0.006847722455859184, -0.01891089789569378, 0.015064320527017117, -0.014064975082874298, -0.06557465344667435, -0.0050200228579342365, -0.03158002719283104, -0.015582723543047905, -0.006110129877924919, 0.0066884052939713, 0.048264030367136, 0.007184804417192936, -0.07269550859928131, 0.012314964085817337, 0.06752266734838486, 0.0369388684630394, -0.02807936817407608, 0.03149860352277756, 0.03111545741558075, 0.05501444637775421, -0.008591018617153168, -0.09005022048950195, -0.02949398010969162, -0.00730098458006978, 0.0077128056436777115, 0.09922753274440765, 0.059822969138622284, 0.018744584172964096, 0.04598494619131088, 0.0585513673722744, 0.0223399605602026, -0.10259361565113068, -0.06040562689304352, 0.10926660150289536, 0.0284910649061203, 0.030670657753944397, -0.015135136432945728, -0.05035930499434471, -0.027013787999749184, -0.07618562132120132, -0.011630327440798283, -0.05206133797764778, 0.09954787790775299, 0.020263245329260826, -0.030589189380407333, -0.01288579311221838, 0.06900955736637115, -0.025277864187955856, 0.0441678985953331, -0.0011802923399955034, -0.006697455886751413, -0.025180771946907043, -0.09850320965051651, -0.05592953786253929, -1.949635952769313e-05, -0.05033240467309952, -0.019515421241521835, 0.0018537082942202687, -2.047430847085252e-08, 0.006107466295361519, 0.012914673425257206, 0.008927502669394016, -0.08149203658103943, 0.03614788502454758, 0.02475021593272686, -0.03178869187831879, 0.03165721893310547, -0.013605521060526371, -0.025946199893951416, 0.05253973230719566, 0.041078805923461914, 0.03368184715509415, -0.03237762302160263, 0.06341931223869324, -0.057850178331136703, 0.023467937484383583, -0.005448203068226576, -0.005410296842455864, -0.002577647566795349, 0.012015728279948235, 0.039440203458070755, 0.010188020765781403, 0.054807212203741074, 0.011527790687978268, 0.0015425708843395114, -0.018151244148612022, 0.03664489462971687, -0.07301980257034302, -0.0360662080347538, -0.06600602716207504, 0.05265358090400696, 0.0027339544612914324, -0.04311543330550194, 0.1267237812280655, -0.08962588012218475, 0.11519454419612885, -0.029792891815304756, -0.029274655506014824, -0.024018006399273872, -0.04484812915325165, 0.03349068760871887, -0.014972776174545288, 0.024420270696282387, 0.04406357184052467, 0.09966620802879333, 0.018153676763176918, -0.025939803570508957, 0.030217649415135384, 0.011330456472933292, 0.07043527066707611, 0.034331779927015305, -0.022376012057065964, 0.04512613266706467, 0.01514021959155798, 0.0056885951198637486, -0.058902084827423096, -0.09230973571538925, -0.09563400596380234, -0.0036592334508895874, 0.08867210149765015, -0.0540606714785099, 0.018345873802900314, -0.01648210734128952]\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "# The embeddings model takes a text as an input and outputs a list of floats\n",
    "text = \"My name is Jerry and I am looking for a senior data scientist or machine learning engineer job\"\n",
    "text_embedding = embeddings_hugging_face.embed_query(text)\n",
    "\n",
    "print(text_embedding)\n",
    "print(len(text_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# Load API credentials\n",
    "with open('api_key.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "os.environ['OPENAI_API_KEY'] = config['OPEN_AI_KEY']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = config['HUGGING_FACE_TOKEN_KEY']\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models, Prompts and Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model = \"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0.5\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI stands for Artificial Intelligence. It refers to the development of computer systems or machines that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and problem-solving. AI aims to create intelligent machines that can learn, reason, and adapt to new situations, ultimately mimicking or augmenting human intelligence. AI can be categorized into two types: narrow AI, which is designed for specific tasks, and general AI, which has the ability to understand, learn, and apply knowledge across various domains.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_1 = \"\"\"\n",
    "I am really sad that people are facing massive layoffs... \\\n",
    "The same situation may happen to me as well. \\\n",
    "The best you can do is to stay positive and work hard!!! \\\n",
    "The future will be better!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"\n",
    "Canada English \\ \n",
    "into a positive tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the text that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: '''{paragraph_1}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is \n",
      "Canada English \\ \n",
      "into a positive tone\n",
      ".\n",
      "text: '''\n",
      "I am really sad that people are facing massive layoffs... The same situation may happen to me as well. The best you can do is to stay positive and work hard!!! The future will be better!!!\n",
      "'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt=prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.3)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt (template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# define the template\n",
    "# this template_1 is a translate template\n",
    "template_1 = \"\"\"\n",
    "translate the text that in delimiated by double quos into a style that is {style}. \\\n",
    "text: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    template = template_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the input string with 1) text 2) style into the prompt template\n",
    "paragraph_1 = \"\"\"\n",
    "I really hate school!!! \\\n",
    "I want to fk you all off, school sucks!!! \\\n",
    "\"\"\"\n",
    "style = \"\"\"\n",
    "English \\\n",
    "in a rude and disrespectful tone\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_msg = prompt_template.format_messages(\n",
    "    style = style,\n",
    "    text = paragraph_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_response = chat(prompt_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I absolutely despise school!!! I couldn't care less about any of you, school is absolutely garbage!!!\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the content back\n",
    "prompt_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse (can treat it as the reverse of prompt, to extract the key information from the paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# to parse the review/comment with parse template to extract the key entity\n",
    "\n",
    "\n",
    " # model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# parse template\n",
    "template_2 = \"\"\"\n",
    "Extract the key information delimiated by double quos into a JSON with following keys: \\\n",
    "professor's_name,\n",
    "professor_rating,\n",
    "course_level,\n",
    "course_description,\n",
    "pass_rate\n",
    "text: \"{text}\"\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# enforce the parse schema in the template\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "schema_list = [\"professor's_name\", \"professor_rating\", \"course_level\", \"course_description\", \"pass_rate\"]\n",
    "response_schema = []\n",
    "\n",
    "for i in schema_list:\n",
    "    item_schema = ResponseSchema(\n",
    "        name=i, \n",
    "        description=i + \"defination in the parser\",\n",
    "        type='string'\n",
    "        )\n",
    "    response_schema.append(item_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "format_instructions = parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    template = template_2,\n",
    "    format_instructions = format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the input string with 1) text into the prompt template\n",
    "paragraph_2 = \"\"\"\n",
    "The course we take is MMA869 which is a course in machine learning and AI teached by professor A. \\\n",
    "Overall the course is not too hard but the content is rich and we learned a lot from machine learning and coding skillsets. \\\n",
    "We would give a 5 star rating to professor A to his great lecturing fashion and easy to understand presentation. \\\n",
    "The course pass rate is around 67%.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_msg = prompt_template.format_messages(\n",
    "    text = paragraph_2,\n",
    "    format_instructions = format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)).\n"
     ]
    }
   ],
   "source": [
    "prompt_response = chat(prompt_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"professor's_name\": \"professor A\",\n",
      "\t\"professor_rating\": \"5 star\",\n",
      "\t\"course_level\": \"MMA869\",\n",
      "\t\"course_description\": \"a course in machine learning and AI\",\n",
      "\t\"pass_rate\": \"around 67%\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(prompt_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExtract the key information delimiated by double quos into a JSON with following keys: professor\\'s_name,\\nprofessor_rating,\\ncourse_level,\\ncourse_description,\\npass_rate\\ntext: \"\\nThe course we take is MMA869 which is a course in machine learning and AI teached by professor A. Overall the course is not too hard but the content is rich and we learned a lot from machine learning and coding skillsets. We would give a 5 star rating to professor A to his great lecturing fashion and easy to understand presentation. The course pass rate is around 67%.\\n\"\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"professor\\'s_name\": string  // professor\\'s_namedefination in the parser\\n\\t\"professor_rating\": string  // professor_ratingdefination in the parser\\n\\t\"course_level\": string  // course_leveldefination in the parser\\n\\t\"course_description\": string  // course_descriptiondefination in the parser\\n\\t\"pass_rate\": string  // pass_ratedefination in the parser\\n}\\n```\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_msg[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = parser.parse(\n",
    "    prompt_response.content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"professor's_name\": 'professor A',\n",
       " 'professor_rating': '5 star',\n",
       " 'course_level': 'MMA869',\n",
       " 'course_description': 'a course in machine learning and AI',\n",
       " 'pass_rate': 'around 67%'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 star'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('professor_rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\n",
    "basically, how do remeber the previous parts of the conversation and then feed into the LLMs\n",
    "\n",
    "### ConversationBufferMemory\n",
    "This memory allows for storing the messages and then extracts the messages in a variable\n",
    "\n",
    "### ConversationBufferWindowMemory\n",
    "This memory keeps a list of the interactions of the conversation over time. it only uses the last K interactions\n",
    "\n",
    "### ConversationTokenBufferMemory\n",
    "This memory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions\n",
    "\n",
    "### ConversationSummaryMemory\n",
    "This memory create a summary of the conversation overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False # if setting it to True, it will reflect the entire conversation and explanation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Jerry! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input= \"Hi, name is Jerry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi, name is Jerry', additional_kwargs={}, example=False), AIMessage(content='Hello Jerry! How can I assist you today?', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history'), callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "long_string = \"\"\"  \n",
    "This is a long string which generate random 1, 2, 3, 4... \\\n",
    "This is a long string which generate random 1, 2, 3, 4... \\\n",
    "This is a long string which generate random 1, 2, 3, 4... \\\n",
    "This is a long string which generate random 1, 2, 3, 4... \\\n",
    "This is a long string which generate random 1, 2, 3, 4... \\\n",
    "This is a long string which generate random 1, 2, 3, 4... \\\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm= llm,\n",
    "    max_token_limit=400\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},\n",
    "    outputs={\"output\": \"Hello how are you, im fine thank you and you\"}\n",
    "    )\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"What is your name\"},\n",
    "    outputs={\"output\": \"My name is Jerry\"}\n",
    "    )\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"What is on the schedule today\"},\n",
    "    outputs={\"output\": \"Nothing much just chilling\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hello\\nAI: Hello how are you, im fine thank you and you\\nHuman: What is your name\\nAI: My name is Jerry\\nHuman: What is on the schedule today\\nAI: Nothing much just chilling'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we input/store the memory, then we will try create a conversation using the memeory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Jerry.\n"
     ]
    }
   ],
   "source": [
    "# as we call the predict method, it will output the store memeory\n",
    "# print(conversation.predict(input=\"What is your name?????\"))\n",
    "print(conversation.predict(input=\"What is your name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data = {\n",
    "        'Products': ['Queen bed', 'waterproof phone pouch', 'luxury air mattress', \"pillows insert\", \"milk shake\"], \n",
    "        'Review': ['I ordered a queen bed, and it is made from China, dream hacker company', \n",
    "                   'this waterproof phone pouch sucks, the USA people made it, and the company is trash phone pouch', \n",
    "                   'i love this luxury air mattress, it is an italian product made by Amazing italy', \n",
    "                   'this pillows insert is good, it is made by a chinese company called good pillows', \n",
    "                   \"i loved this milk shake!! it is made from Taiwan, company name is 'Big Milk Shake'\"]\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI # model\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate # template\n",
    "from langchain.chains import LLMChain # chain, the combination of LLMs and prompts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.65)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Extract the company name for the following products' review in the text format. \\\n",
    "What is the best name to describe a company that makes {product}. \\\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To extract the company name from product reviews, we need specific examples or the text containing those reviews. Without that information, we cannot provide you with the company names.\\n\\nRegarding the best name to describe a company that makes milkshakes, some possible options could be:\\n\\n1. \"Milkshake Masters\"\\n2. \"Creamy Shake Co.\"\\n3. \"Shake Delights\"\\n4. \"Frosty Shake Factory\"\\n5. \"Dreamy Shake Creations\"\\n6. \"Smoothie Shake Company\"\\n7. \"Shake Bliss\"\\n8. \"Milkshake Magic\"\\n9. \"Chilled Shake Company\"\\n10. \"Shake Paradise\"\\n\\nUltimately, the best name would depend on the company\\'s target audience, branding strategy, and unique selling proposition.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"milk shake\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chains\n",
    "sequential chain is another types of chains. the idea is to combine multiple chains where the output of the one chain \\\n",
    "is the input of the next chain\n",
    "\n",
    "there is two types of sequential chains:\n",
    "1. SimpleSequentialChain: Single input / output\n",
    "2. SequentialChain: multiple inputs / outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SimpleSequentialChain\n__root__\n  Chains used in SimplePipeline should all have one input, got memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None prompt=ChatPromptTemplate(input_variables=[], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='What is the best name to describe a company that makes the milk shake', template_format='f-string', validate_template=True), additional_kwargs={})]) llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None) output_key='text' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} with 0 inputs. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m chain_2 \u001b[39m=\u001b[39m LLMChain(\n\u001b[0;32m     18\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m     19\u001b[0m     prompt\u001b[39m=\u001b[39mprompt_template_2\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39m# combine the 2 chains in a sequential\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m overall_chain \u001b[39m=\u001b[39m SimpleSequentialChain(\n\u001b[0;32m     24\u001b[0m     chains\u001b[39m=\u001b[39;49m[chain_1, chain_2],\n\u001b[0;32m     25\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\langchain\\load\\serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SimpleSequentialChain\n__root__\n  Chains used in SimplePipeline should all have one input, got memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None prompt=ChatPromptTemplate(input_variables=[], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='What is the best name to describe a company that makes the milk shake', template_format='f-string', validate_template=True), additional_kwargs={})]) llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None) output_key='text' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} with 0 inputs. (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# the first chain\n",
    "prompt_template_1 = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes the milk shake\"\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_1\n",
    ")\n",
    "\n",
    "# the second chain\n",
    "prompt_template_2 = ChatPromptTemplate.from_template(\n",
    "    \"What is the product's country is from\"\n",
    ")\n",
    "chain_2 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_2\n",
    ")\n",
    "\n",
    "# combine the 2 chains in a sequential\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[chain_1, chain_2],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain # this is the regular multi-lines chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# chain 1\n",
    "prompt_template_1 = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to chinese: \"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_1,\n",
    "    output_key=\"English_review\"\n",
    ")\n",
    "\n",
    "# chain 2\n",
    "prompt_template_2 = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in one sentence:\"\n",
    "    \"\\n\\n{English_review}\"\n",
    ")\n",
    "\n",
    "chain_2 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_2,\n",
    "    output_key=\"summary\"\n",
    ")\n",
    "\n",
    "# chain 3\n",
    "prompt_template_3 = ChatPromptTemplate.from_template(\n",
    "    \"What Language is the following review:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "\n",
    "chain_3 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_3,\n",
    "    output_key=\"language\"\n",
    ")\n",
    "\n",
    "# chain 4\n",
    "prompt_template_4 = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following\"\n",
    "    \"suumary in the specified language\"\n",
    "    \"\\n\\nSuumary: {summary} \\n\\nLanguage: {language}\"\n",
    ")\n",
    "\n",
    "chain_4 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_4,\n",
    "    output_key=\"followup_message\"\n",
    ")\n",
    "\n",
    "# overall chain: input = Review\n",
    "# and output = English_review, summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_1,\n",
    "            chain_2,\n",
    "            chain_3,\n",
    "            chain_4],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_review\", \"summary\", \"followup_message\"],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Products</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen bed</td>\n",
       "      <td>I ordered a queen bed, and it is made from Chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waterproof phone pouch</td>\n",
       "      <td>this waterproof phone pouch sucks, the USA peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>luxury air mattress</td>\n",
       "      <td>i love this luxury air mattress, it is an ital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pillows insert</td>\n",
       "      <td>this pillows insert is good, it is made by a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>milk shake</td>\n",
       "      <td>i loved this milk shake!! it is made from Taiw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Products                                             Review\n",
       "0               Queen bed  I ordered a queen bed, and it is made from Chi...\n",
       "1  waterproof phone pouch  this waterproof phone pouch sucks, the USA peo...\n",
       "2     luxury air mattress  i love this luxury air mattress, it is an ital...\n",
       "3          pillows insert  this pillows insert is good, it is made by a c...\n",
       "4              milk shake  i loved this milk shake!! it is made from Taiw..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tS7EHZnbhbL05zLRtb31ujNQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': 'this waterproof phone pouch sucks, the USA people made it, and the company is trash phone pouch',\n",
       " 'English_review': '这个防水手机袋真糟糕，是美国人制造的，这个公司的手机袋简直糟糕透顶。',\n",
       " 'summary': \"This waterproof phone bag is terrible, it is made by Americans and the company's phone bags are simply awful.\",\n",
       " 'followup_message': 'Response: Thank you for your feedback on the waterproof phone bag. We appreciate your opinion, although we would like to clarify that the nationality of the manufacturer does not necessarily determine the quality of their products. However, we apologize if our phone bags did not meet your expectations. We continuously strive to improve our products and your comments will be taken into consideration for future enhancements. If you have any specific issues with our phone bag, please feel free to share them, as we would be more than happy to assist you further.'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[1]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route chain template\n",
    "\n",
    "# model\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# chain the prompt up\n",
    "destination_chains =  {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"],\n",
    "    prompt = ChatOpenAI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
